---
title: "LTAW_data_cleaning"
author: "Bernice Cheung"
date: "2023-08-09"
output:
  html_document:
    code_folding: hide
    df_print: paged
    highlight: tango
    theme: united
    toc: yes
    toc_float:
      collapsed: yes
      smooth_scroll: yes
  github_document:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
options(scipen=999)
```

# load packages
```{r b_edit}
if(!require('pacman')) {
	install.packages('pacman')
}

# added here and psych package
pacman::p_load(tidyverse, labelled, devtools, haven, expss, DT, qwraps2, remotes, readxl, retidytext, openxlsx, reactable, reactablefmtr, ggwordcloud, topicmodels,here,psych,rio, install = TRUE)
```

# define aesthetics
```{r}
palette = c("#772e25", "#c44536", "#ee9b00", "#197278", "#283d3b", "#9CC5A1", "#ADA7C9", "grey50")
palette_type = c("#c44536", "#ee9b00", "#197278")
palette_pilot = c("#c44536", "#197278")
palette_sentiment = c(palette[2], palette[4])
plot_aes = theme_minimal() +
  theme(legend.position = "top",
        legend.text = element_text(size = 8),
        text = element_text(size = 12, family = "Futura Medium"),
        axis.text = element_text(color = "black"),
        axis.ticks.y = element_blank())
```

# define variables & directory

```{r}
survey_name <- "ltaw"
raw_file_name <- "Let's Talk About Well-Being_August 2, 2023_07.59.sav"
prolific_user_file <- "prolific_export_639e73b38884ae6555e593ca.csv"
prolific_id_length <- 24
#correct responses for attention checks
attention1_correct <- 3
attention2_correct <- 9

first_survey_var <- "SWLS_1"
last_survey_var <- "SRIS12_12"

# outputs from manually check prolific status for unmatched cases
prolific_check_outputs <- "prolific_check_df - prolific_check_df.csv"
```

# define functions
```{r}
value_between <- function(vec, start, end){
  
  result <- vec[(which(vec == start)):(which(vec == end))]
  
  return(result)
}
```

# Basic Cleaning {.tabset}
1. load & deidentify data 
  1a. Prolific status check -- merge & address data deletion
2. deduplication + duplicate summary
3. summarize consent, terms & NDA (where applicable) + additional participant "electives" (e.g., here "content_recruit")
4. attention checks & recording test
5. summarize qualtrics completion (retention/completion rates) 
5. codebook 
6. write cleaned csv + dictionary 

## load & de-identify Qualtrics data 
*notes: (1) this is pretty specific to prolific studies, can be adapted as needed [see calarts for example] (2)select line @ end will need to be adapted slightly based on any additional PII collected [see calarts]*

After excluding testing cases, 3014 observations are included in the dataset
```{r clean test & deidentify}
#survey <- read_sav("/Users/kristina/Downloads/Let's Talk About Well-Being_August 2, 2023_07.59.sav")
survey <- import(here("inputs", raw_file_name))
survey$survey_name <- survey_name

#missing ID check
plyr::count(survey$PROLIFIC_PID=="")
survey$PROLIFIC_PID <- ifelse(survey$PROLIFIC_PID == "", survey$Prolific_ID, survey$PROLIFIC_PID)
plyr::count(survey$PROLIFIC_PID=="")
survey$PROLIFIC_PID <- ifelse(survey$PROLIFIC_PID == "", survey$ID_check, survey$PROLIFIC_PID)
plyr::count(survey$PROLIFIC_PID=="")

subset(survey, survey$PROLIFIC_PID=="") #confirm any missing IDs are blank data

survey_filtered <- survey %>%
  filter(!PROLIFIC_PID == "") %>% # remove test responses
  filter(nchar(PROLIFIC_PID) == prolific_id_length) %>%
  filter(!DistributionChannel == "Preview") %>% # remove incomplete responses
  select(-c(StartDate, EndDate, Status, Finished, RecipientLastName,  IPAddress,RecipientFirstName, RecipientEmail, LocationLatitude, LocationLongitude, DistributionChannel, UserLanguage)) %>% #remove cols #de-identify
  dplyr::rename(userid = PROLIFIC_PID) %>% # standardize userIDs
  select(userid, everything()) %>% # move userID to the front
  mutate(across(where(is.character), ~na_if(., ""))) #convert empty cell into NAs

# add labels 
attr(survey_filtered$userid, "label") <- "User IDs that can be used for data merging"
```

## Prolific status check

Records from 2573 participants are included in the prolific dataset. Among them, 2 participants don't have matching data from the survey dataset, and both of them revoked their consent. 392 observations from 387 participants in the survey dataset don't have matching data in the prolific dataset. Their prolific_status will be marked as "TIMEOUT"
```{r prolific merge}
#prolific <- read.csv("/Users/kristina/Downloads/prolific_export_639e73b38884ae6555e593ca.csv")
prolific <- read.csv(here("inputs",prolific_user_file))

# tidy prolific dataset
prolific_tidy <- prolific %>%
  rename_all(~paste0(., "_prolific")) %>% # add suffix
  dplyr::rename(userid = Participant.id_prolific) %>% # standardize user IDs name
  dplyr::mutate(across(where(is.character),  # recode null data into NA 
                       ~  case_when(
                              . == "DATA_EXPIRED" ~ NA,
                              . == "Prefer not to say" ~ NA,
                              TRUE ~ .)))

# merge with survey data
survey_merged <- survey_filtered %>% left_join(prolific_tidy, by = "userid") %>%
  mutate(Status_prolific = replace_na(Status_prolific, "TIMEDOUT"))

# check participants from the prolific file that can't match in the survey data file
prolific_notmatch <- anti_join(prolific_tidy, survey_filtered, by = "userid")
# check observations from the survey data file that can't match in the prolific file
survey_notmatch <- anti_join(survey_filtered, prolific_tidy, by = "userid")
# check number of subjects missing from prolific
length(unique(survey_notmatch$userid))

# flag non-matched observations
prolific_notmatch_id <- unique(prolific_notmatch$userid)
survey_notmatch_id <- unique(survey_notmatch$userid)

survey_match_f <- survey_merged %>%
  mutate(flag_nomatch_prolific = case_when(userid %in% prolific_notmatch_id ~ TRUE,
                                   TRUE ~ FALSE), 
         flag_nomatch_survey = case_when(userid %in% survey_notmatch_id ~ TRUE,
                                   TRUE ~ FALSE))

attr(survey_match_f$flag_nomatch_prolific, "label") <- "TRUE if the observation is included in the prolific file but not in the survey data file"
attr(survey_match_f$flag_nomatch_survey, "label") <- "TRUE if the observation is included in the survey data file but not in the prolific file"
```

## Consent & Terms
68 participants (no duplicate IDs) revoked consent or did not complete consent upon entering the study. These participant IDs are removed, leaving 2878 total observations. 

70 participants returned the study in Prolific, effectively revoking consent. There are various reasons for a user to return their survey, which are detailed below where possible. Data coming from these users is removed from the dataset here.  
*notes for iteration: (1) will want to probably merge other data (moments etc) before this step so all data is removed/accounted for (2) can add details via excel import/merge of notes?*
```{r clean consent} 
# remove data from revoked consent
survey_cleaned = survey_match_f %>%
  filter(!Status_prolific == "RETURNED") %>% # remove returned surveys
  filter(Consent_confirm == 1) # only include data from confirmed consent

#summarize returned responses
data_remove <- survey_match_f %>%
  select(userid, Status_prolific, Submission.id_prolific) %>%
  unique() %>%
  filter(Status_prolific =="RETURNED") %>%
  group_by(userid, Status_prolific,Submission.id_prolific) %>%
  mutate(detials = "") 
data_remove<- data_remove[order(data_remove$userid),]

data_remove 

survey_match_f %>%
  select(userid, Status_prolific) %>%
  group_by(Status_prolific) %>%
  mutate(count = length(Status_prolific)) %>%
  dplyr::summarize(n = n()) %>%
  spread(Status_prolific, n) %>%
  mutate(percent_approved = round((APPROVED/ nrow(survey_match_f)) * 100, 1)) %>%
  mutate(percent_available = round(((APPROVED+TIMEDOUT)/nrow(survey_match_f)) * 100, 1))
```

## Summarize qualtrics: consent, terms agreement, NDA, attention checks, recording tests, etc {.tabset}

### deduplication & duplicates summary
There were 100 total duplicate reponses, from 49 unique participants, **thus in this study, data was collected from 2827 unique participants**. Duplication in survey data could have happened if the participant accessed and started the Qualtrics study multiple times. Here, we retain all duplicated data and flag each response, responses are also flagged based on completion. See below for details.

```{r flag duplication}
survey_dupe_f <- survey_cleaned %>%
  group_by(userid) %>%
  mutate(flag_dupe_survey = duplicated(userid) | rev(duplicated(rev(userid)))) %>% # flag rows with identical userid
  dplyr::mutate(
  max_progress = max(Progress),
  distinct_progress = n_distinct(Progress),
  total_rows = nrow(.)) %>%
  mutate(
    flag_most_complete_survey = case_when(
      # Unique userid cases
      flag_dupe_survey == F ~ "TRUE",
      
      # Duplicated userid but identical progress
      flag_dupe_survey == T & total_rows > 1 & distinct_progress == 1 ~ "SAME",
      
      # Duplicated userid with non-identical progress
      flag_dupe_survey == T & Progress == max_progress & distinct_progress > 1 ~ "TRUE",
      flag_dupe_survey == T & Progress != max_progress & distinct_progress > 1 ~ "FALSE",
      
      TRUE ~ NA_character_
    )
  ) %>%
  select(-max_progress, -distinct_progress, -total_rows) %>%
  ungroup()

attr(survey_dupe_f$flag_dupe_survey, "label") <- "TRUE: data from userid that have more than 1 occurrence; FALSE: data from unique userid"
attr(survey_dupe_f$flag_most_complete_survey, "label") <- "TRUE: data from unique userid or has the highest progress among duplicated responses; FALSE: duplicated responses that don’t have the highest progress; SAME: duplicated response that share the same progress"

table(survey_dupe_f$flag_dupe_survey)
length(unique(survey_dupe_f[survey_dupe_f$flag_dupe_survey == TRUE,]$userid))
```


### Failed recording test
35 participants failed the recording test, 204 had missing data. 

The `flag_failed_recording_test` marks if the parcipants failed the recording test, retaining NA/missing. 
```{r flag failed testing}

survey_dupe_f %>%
  select(survey_name, userid, Recording_test) %>%
  unique() %>%
  gather(Recording_test, value, Recording_test) %>%
  filter(is.na(value)|value!=1) %>%
  group_by(Recording_test, survey_name, value) %>%
  dplyr::summarize(n = n())

survey_record_f <- survey_dupe_f %>%
  mutate(flag_failed_recording_test = case_when(Recording_test == 0 ~ TRUE, 
                                                Recording_test == 1 ~ FALSE,
                                                TRUE ~ NA))

attr(survey_record_f$flag_failed_recording_test, "label") <- "TRUE if participants failed teh recording test. NAs were retained"

```


### Failed attention check
Here, there were 2 attention checks, correct responses for each are noted in the attention1_correct & attention2_correct columns, respectively. 

`flag_failed_attention` = FALSE if both attention checks passed
`flag_failed_attention` = FALSE if 1 attention check passed + 1 missing 
`flag_failed_attention` = TRUE if at least 1 attention checks failed
`flag_failed_attention` = NA if both attention checks are missing

`flag_failed_attention_n` = total number of attention checks failed (NA not counted as failure)

61 observations had 1 failed attention check question, 2 observations had 2 (failed both questions), 173 had at least 1 attention check missing data without an explicit failure (so either passed 1 or both NA)
```{r flag failed attention checks}

# add a flag for failed attention check question and a flag for the number of attention check
survey_attention_f <- survey_record_f %>%
  mutate(flag_failed_attention = case_when(
    Attention1 == attention1_correct & Attention2 == attention2_correct ~ FALSE, # FALSE if both attention check questions were correct 
    Attention1 != attention1_correct | Attention2 != attention2_correct ~ TRUE,  # TRUE if get at least 1 attention check question was wrong
    is.na(Attention1) & is.na(Attention2) ~ NA, # NA if both attention check questions are missing
    TRUE ~ FALSE # FALSE if there's one question correct and one quetion missing
  )) %>%
  rowwise() %>%
  mutate(flag_failed_attention_n = case_when(
    flag_failed_attention == T & (is.na(Attention1) | is.na(Attention2)) ~ 1, # if there's a missing data, and an incorrect answer, mark 1
    TRUE ~ (Attention1 != attention1_correct) + (Attention2 != attention2_correct) # if no missing attention check question, add the number of failed question
  )) %>%
  ungroup() %>%
  as.data.frame()

table(survey_attention_f$flag_failed_attention_n, useNA = "ifany")

# add label
attr(survey_attention_f$flag_failed_attention, "label") <- "TRUE if at least 1 attention checks failed; FALSE if 1 attention check passed + 1 missing or failed both; NA if both attention checks are missing"
attr(survey_attention_f$flag_failed_attention_n, "label") <- "total number of attention checks failed (NA not counted as failure)"
```


### Content recruit/release 
1956 (~68%) of participants responded "yes" to the question *"Would you be interested in being contacted about a later Prolific study where you provide voice recordings for us to use in our company’s future projects?"* See below for details.
```{r screen consent recruit}
n_obs <- nrow(survey_attention_f)

survey_attention_f %>%
  select(survey_name, userid, content_recruit) %>%
  unique() %>%
  gather(content_recruit, value, content_recruit) %>%
  filter(value ==1) %>%
  dplyr::summarize(n = n()) %>%
  mutate(percent_recruited = round((n / n_obs) * 100, 1))

content_recruit <- survey_attention_f %>%
  select(survey_name, userid, content_recruit) %>%
  unique() %>%
  gather(content_recruit, value, content_recruit) %>%
  filter(value ==1) %>%
  group_by(content_recruit, survey_name) 
DT::datatable(content_recruit)
```

## Additional data screening {.tabset}

### Missing all survey data

129 observation didn't provide any survey item data
```{r flag no survey data}
# extract survey item variables
survey_var <- value_between(colnames(survey_cleaned), first_survey_var, last_survey_var)


survey_survey_f <- survey_attention_f %>%
  mutate(flag_no_survey = (rowSums(is.na(dplyr::select(., all_of(survey_var)))) == length(dplyr::select(., all_of(survey_var)))))

table(survey_survey_f$flag_no_survey, useNA = "ifany")

attr(survey_survey_f$flag_no_survey, "label") <- "TRUE if participants didn't provide any survey data; FALSE if at least 1 survey item was recorded"
```

### Data quality: repetitive responses in a sequence

We want to flag participants who provided identical responses in a long sequence. The highest percentage of identical sequence is 35%, which is within a reasonable range
```{r screen repetitive responses}
# calculate the number of survey item 
item_n <- ncol(select(survey_survey_f, all_of(survey_var)))

survey_screen <- survey_survey_f %>%
  mutate(max_identical = apply(select(., all_of(survey_var)), 1, function(x) {max(rle(x)$lengths)}), 
         max_identical_perc = round((max_identical/item_n)*100))

describe(survey_screen$max_identical_perc)

attr(survey_screen$max_identical, "label") <- "The number if identical survey responses in a seqeunce"
attr(survey_screen$max_identical_perc, "label") <- "The perceptage of identical survey responses in a seqeunce out of all survey items"
```


### Study duration

check the distribution of survey duration among those who completed the survey within 2 hours. The expected completion time was 45 minutes. 
```{r screen duration}
# subset completed observations
survey_complete <- survey_screen %>%
  filter(flag_most_complete_survey!="FALSE") %>% #filter dupes & no survey data
  mutate(Progress = ifelse(Progress == 100, "complete", "incomplete")) %>%
  filter(Progress == "complete") %>%
  mutate(duration = Duration__in_seconds_/60) # transform survey durection into minutes

# check overall duration
describe(survey_complete$duration)

# check duration distribution within 2 hours
hist(survey_complete[survey_complete$duration < 120,]$duration)

```

To further examine those responses with a completion time of less than 10 minutes, we pair these data with other data screening results. Considering all screening, the data quality here looks fine. Therefore, no flag is necessary based on survey duration. 
```{r screen duration cont}
survey_complete %>%
  filter(duration < 10) %>%
  select(flag_no_survey, duration, flag_failed_recording_test, flag_failed_attention_n, max_identical_perc)
```

## Summarize data screening results

Based on all data screening efforts, 2692 observations are recommended to include in the analysis. Criteria include:
- at least some survey data
- did not failed both attention check questions 
- is the most completed survey if there were duplicates. 

These column identified as "recommended for analyses" are flagged in the `flag_rec_include` column
```{r flag recommended obaservation}
  
# mark the observation
survey_tidy <- survey_screen %>%
  mutate(flag_rec_include = case_when(flag_most_complete_survey != FALSE & flag_no_survey == FALSE & flag_failed_attention_n != 2 ~ TRUE,
          TRUE ~ FALSE))

table(survey_tidy$flag_rec_include, useNA = "ifany")

# add labels
attr(survey_tidy$flag_rec_include, "label") <- "TRUE if WB team recommend to include this observation. Criteria include:- at least some survey data
- did not failed both attention check questions 
- is the most completed survey if there were duplicates. "
```

# exclude data based on manually checking prolific status

## select data that need to be manually checked on prolific

These are the observations in the survey data that are not included in the downloaded prolific dataset but provide at least some valid data. We have 233 observations from 232 participants that need to be checked on prolific
```{r}
survey_tidy <- survey_tidy %>%
  mutate(flag_prolific_check = case_when(flag_nomatch_survey == TRUE & flag_no_survey == FALSE ~ TRUE,
                                         TRUE ~ FALSE))

prolific_check_df <- filter(survey_tidy, flag_prolific_check == TRUE) %>%
  select(userid, flag_nomatch_survey, flag_no_survey, Progress, Duration__in_seconds_, Status_prolific)

attr(survey_tidy$flag_prolific_check, "label") <- "TRUE if the observation includes at least one valid survey item response  but can't find a match in the prolific dataset"

#write.csv(prolific_check_df, here("outputs", "prolific_check_df.csv"), row.names = F)
```

## Exclude data

After manually excluded 189 observations with revoked consent, the cleaned dataset include 2689 observations. 
```{r}
# load prolific manual check results 
prolific_check_results <- read.csv(here("inputs", "prolific_check_df - prolific_check_df.csv"))

nrow(filter(prolific_check_results, anne_check == "RETURNED"))

# exclude observations with revoked consent
survey_manual_exclude <- survey_tidy %>%
  left_join(select(prolific_check_results, userid, Duration__in_seconds_, anne_check), by = c("userid", "Duration__in_seconds_")) %>%
  mutate(anne_check = replace_na(anne_check, "NOCHECK")) %>%
  filter(anne_check != "RETURNED") %>%
  rename(flag_prolific_manual_check = anne_check)

attr(survey_manual_exclude$flag_prolific_manual_check, 'label') <- "indicates results from manually checking status on Prolific; If it's RETURNED, excluded from the dataset; If it's TIMEOUT, keep in the dataset; if it's NOCHECK, the observation doesn't need manual inspection. "
```

# Summarize qualtrics completion info {.tabset}

## full data
Overall, there are 2504 cases of complete data, when accounting for duplicates, ~94.8% completion overall. 
```{r screen completion}
survey_manual_exclude %>%
  select(survey_name, userid, Progress) %>%
  mutate(Progress = ifelse(Progress == 100, "complete", "incomplete")) %>%
  group_by(Progress) %>%
    mutate(count = length(survey_name)) %>%
  dplyr::summarize(n = n()) %>%
  spread(Progress, n) %>%
  mutate(percent_complete = round((complete / (complete+incomplete)) * 100, 1))

```
## no dupes
```{r screen completion no dup}
survey_manual_exclude %>%
  select(survey_name, userid, Progress, flag_most_complete_survey) %>%
  filter(flag_most_complete_survey!="FALSE") %>% #filter dupes 
  mutate(Progress = ifelse(Progress == 100, "complete", "incomplete")) %>%
  group_by(Progress) %>%
    mutate(count = length(survey_name)) %>%
  dplyr::summarize(n = n()) %>%
  spread(Progress, n) %>%
  mutate(percent_complete = round((complete / (complete+incomplete)) * 100, 1))

```

# code & score qualtrics data {.tabset}
1. recode demographics to character
2. psychological surveys

## recode numbers of the categorical variables into characters
```{r recode categorical var}
# generate a dictionary
survey_tidy_dict <- create_dictionary(survey_manual_exclude, remove_repeated = F, use_references = F)

# transform the dictionary into a list 
dict_list <- split(select(survey_tidy_dict, -meta), survey_tidy_dict$variable)

# extract all variable numbers for the "select all that apply" questions
varname <- names(dict_list)

# all categorical variables from the survey item
demo_idx <- which(grepl("Demo_", varname))
response_idx <- which(grepl("^ResourceHelp_", varname) & !grepl("_TEXT$", varname))
content_idx <- which(grepl("^ContentHelp_", varname) & !grepl("_TEXT$", varname))
languish_idx <- which(grepl("^Languishing_", varname))
doc_tool_idx <- which(grepl("^Document_Tools_", varname) & !grepl("_TEXT$", varname))
doc_benefic_idx <- which(grepl("^Document_Benefits", varname) & !grepl("_TEXT$", varname))
doc_freq_idx <- which(varname == "Document_Freq")

multi_resposne_idx <- c(demo_idx, response_idx,content_idx,languish_idx,doc_tool_idx,doc_benefic_idx, doc_freq_idx)

# subset these variables from the dictionary list
recode_dict_list <- dict_list[multi_resposne_idx]

# replace response with variable label
survey_catvar_r <- survey_manual_exclude
survey_catvar_r[names(recode_dict_list)] <- Map(function(x, y) {
    tmp <- with(y,  setNames(label, value)[as.character(x)])
     tmp[is.na(tmp)] <- x[is.na(tmp)]
     tmp}, survey_manual_exclude[names(recode_dict_list)], recode_dict_list)
```

## coalesce multiple responses

```{r coalesce multiple responses}
survey_comb_r <- survey_catvar_r %>%
   mutate(
          resource_help_combine = case_when(
            rowSums(!is.na(select(., ResourceHelp_1:ResourceHelp_9))) == 0 ~ NA_character_,
            rowSums(!is.na(select(., ResourceHelp_1:ResourceHelp_9))) == 1 ~ as.character(do.call(coalesce, select(., ResourceHelp_1:ResourceHelp_9))),
            rowSums(!is.na(select(., ResourceHelp_1:ResourceHelp_9))) > 1 ~ "multi"), 
          
          content_help_combine = case_when(
            rowSums(!is.na(select(., ContentHelp_1:ContentHelp_12))) == 0 ~ NA_character_,
            rowSums(!is.na(select(., ContentHelp_1:ContentHelp_12))) == 1 ~ as.character(do.call(coalesce, select(., ContentHelp_1:ContentHelp_12))),
            rowSums(!is.na(select(., ContentHelp_1:ContentHelp_12))) > 1 ~ "multiple"), 
          
          languishing_combine = case_when(
            rowSums(!is.na(select(., Languishing_1:Languishing_10))) == 0 ~ NA_character_,
            rowSums(!is.na(select(., Languishing_1:Languishing_10))) == 1 ~ as.character(do.call(coalesce, select(., Languishing_1:Languishing_10))),
            rowSums(!is.na(select(., Languishing_1:Languishing_10))) > 1 ~ "multiple"),
          
          document_tools_combine = case_when(
            rowSums(!is.na(select(., Document_Tools_1:Document_Tools_7))) == 0 ~ NA_character_,
            rowSums(!is.na(select(., Document_Tools_1:Document_Tools_7))) == 1 ~ as.character(do.call(coalesce, select(., Document_Tools_1:Document_Tools_7))),
            rowSums(!is.na(select(., Document_Tools_1:Document_Tools_7))) > 1 ~ "multiple"),
          
          document_benefits_combine = case_when(
            rowSums(!is.na(select(., Document_Benefits_1:Document_Benefits_11))) == 0 ~ NA_character_,
            rowSums(!is.na(select(., Document_Benefits_1:Document_Benefits_11))) == 1 ~ as.character(do.call(coalesce, select(., Document_Benefits_1:Document_Benefits_11))),
            rowSums(!is.na(select(., Document_Benefits_1:Document_Benefits_11))) > 1 ~ "multiple"))

# add labels to new data
attr(survey_comb_r$resource_help_combine, "label") <- "coalesced responses of the question 'What do you think would help you with meeting that goal?'"
attr(survey_comb_r$content_help_combine, "label") <- "coalesced responses of the question 'Of the following topic areas, which do you feel you could use the most help with?'"
attr(survey_comb_r$languishing_combine, "label") <- "coalesced responses of the question 'In the last month, have you felt like any of the following?'"
attr(survey_comb_r$document_tools_combine, "label") <- "coalesced responses of the question 'What tools do you use to journal or document your life?"
attr(survey_comb_r$document_benefits_combine, "label") <- "coalesced responses of the question 'What do you think would help you with meeting that goal?'"
attr(survey_comb_r$resource_help_combine, "label") <- "coalesced responses of the question 'Do you experience any of the following benefits when you document your life?'"
```

## demographics
recoded from numeric to character; parse race/ethnicity variables 
```{r coalesce demographics}
survey_demo_r <- survey_comb_r %>%
  mutate(Demo_ethnicity__99 = NA) %>% # "I prefer not to answer" into NA
  mutate(demo_race_combine = case_when( # coalesce race
    rowSums(!is.na(select(., Demo_ethnicity_1:Demo_ethnicity_7, -Demo_ethnicity_3))) == 0 ~ NA_character_,
    rowSums(!is.na(select(., Demo_ethnicity_1:Demo_ethnicity_7, -Demo_ethnicity_3))) == 1 ~ as.character(do.call(coalesce, select(., Demo_ethnicity_1:Demo_ethnicity_7, -Demo_ethnicity_3))),
    rowSums(!is.na(select(., Demo_ethnicity_1:Demo_ethnicity_7, -Demo_ethnicity_3))) > 1 ~ "multiracial")) %>% # ethnicity is not considered for multiracial
  dplyr::mutate(demo_ethnicity = Demo_ethnicity_3) # differciate between race and ethnicity

attr(survey_demo_r$demo_race_combine, "label") <- "coalesced responses of the question 'Please specify your race and/or ethnicity.''multiracial' if multiple categories were selected. (Latinx/Hispanic was not included)"
attr(survey_demo_r$demo_ethnicity, "label") <- "whether particpants self-identify as Latinx/Hispanic, Is not coalesed with racial categories"
```

## SWLS
A total score is calculated by adding up the scores for each item. The possible range of scores is 5-35, with a score of 20 representing a neutral point on the scale. Scores between 5-9 indicate the respondent is extremely dissatisfied with life, whereas scores between 31-35 indicate the respondent is extremely satisfied.
```{r}
# calculate life satisfaction sum score & recode categories
surveys_scored <- survey_demo_r %>%
  mutate(SWLS_tot = rowSums(select(., contains("SWLS_"))), # compute the sum score
         SWLS_tot_f = case_when(5 <= SWLS_tot & SWLS_tot <= 9 ~ "Extremely Dissatisfied",  # categorize the sum score
                                10 <= SWLS_tot & SWLS_tot <= 14 ~ "Dissatisfied",
                                15 <= SWLS_tot & SWLS_tot <= 19 ~ "Slightly Dissatisfied",
                                SWLS_tot == 20 ~ "Neutral",
                                21 <= SWLS_tot & SWLS_tot <= 25 ~ "Slightly Satisfied",
                                26 <= SWLS_tot & SWLS_tot <= 30 ~ "Satisfied",
                                31 <= SWLS_tot & SWLS_tot <= 35 ~ "Extremely Satisfied")) %>%
  mutate(SWLS_tot_f = as.factor(SWLS_tot_f))

# add variable labels
attr(surveys_scored$SWLS_tot, "label") <- "total life satisfaction score"
attr(surveys_scored$SWLS_tot_f, "label")  <- "life satisfaction categories"

```

## PERMA
Positive affect is calculated as the average of PERMA positive emotions items 1,3,6
Negative affect is calculated as the average of PERMA positive emotions items 2,4,5
```{r}
surveys_scored <- cbind(surveys_scored, PERMA_pos = rowSums(surveys_scored[, grepl("PERMA_1|PERMA_3|PERMA_6", names(surveys_scored))])/3)
attr(surveys_scored$PERMA_pos, "label") = "PERMA Positive Affect"

surveys_scored <- cbind(surveys_scored, PERMA_neg = rowSums(surveys_scored[, grepl("PERMA_2|PERMA_4|PERMA_5", names(surveys_scored))])/3)
attr(surveys_scored$PERMA_neg, "label") = "PERMA Negative Affect"

```

## PWB
Q1, Q2, Q3, Q8, Q9, Q11, Q12, Q13, Q17, and Q18 should be reverse-scored.
**NOTE: response options were anchored incorrectly, reverse score opposite of what is noted (items 4,5,6,7,10,14,15,16 reversed)**
Reverse-scored items are worded in the opposite direction of what the scale is
measuring. The formula for reverse-scoring an item is:
((Number of scale points) + 1) - (Respondent’s answer)

The Autonomy subscale items are Q15R,Q17,Q18. The Environmental Mastery subscale
items are Q4R, Q8, Q9. The Personal Growth subscale items are Q11, Q12, Q14R. The
Positive Relations with Others subscale items are Q6R, Q13, Q16R. The Purpose in Life subscale items are Q3, Q7R, Q10R. The Self-Acceptance subscale items are Q1, Q2, and Q5R. 

To calculate subscale scores for each participant, sum respondents’ answers to each subscale’s items. Higher scores mean higher levels of psychological well-being.
```{r}
surveys_scored <- surveys_scored %>% #reverse score
  mutate(PWB_4_R = 8 - PWB_4,
         PWB_5_R = 8 - PWB_5,
         PWB_6_R = 8 - PWB_6,
         PWB_7_R = 8 - PWB_7,
         PWB_10_R = 8 - PWB_10,
         PWB_14_R = 8 - PWB_14,
         PWB_15_R = 8 - PWB_15,
         PWB_16_R = 8 - PWB_16)

#autonomy Q15R,Q17,Q18.
surveys_scored <- cbind(surveys_scored, PWB_Autonomy_tot = rowSums(surveys_scored[,  grepl("\\bPWB_15_R\\b|\\bPWB_17\\b|\\bPWB_18\\b", names(surveys_scored))]))

#Environmental Mastery Q4R, Q8, Q9
surveys_scored <- cbind(surveys_scored, PWB_EnvMas_tot = rowSums(surveys_scored[,  grepl("\\bPWB_4_R\\b|\\bPWB_8\\b|\\bPWB_9\\b", names(surveys_scored))]))

#Personal Growth Q11, Q12, Q14R.
surveys_scored <- cbind(surveys_scored, PWB_Growth_tot = rowSums(surveys_scored[,  grepl("\\bPWB_11\\b|\\bPWB_12\\b|\\bPWB_14_R\\b", names(surveys_scored))]))

#Positive Relations with Others Q6R, Q13, Q16R.
surveys_scored <- cbind(surveys_scored, PWB_PosRelations_tot = rowSums(surveys_scored[,  grepl("\\bPWB_6_R\\b|\\bPWB_13\\b|\\bPWB_16_R\\b", names(surveys_scored))]))

#Purpose in Life Q3, Q7R, Q10R. 
#added additional purpose subscale items from longer PWB given study design focused on PURPOSE
surveys_scored <- cbind(surveys_scored, PWB_Purpose_tot = rowSums(surveys_scored[,  grepl("\\bPWB_3\\b|\\bPWB_7_R\\b|\\bPWB_10_R\\b", names(surveys_scored))]))
#surveys_scored <- cbind(surveys_scored, PWB_PurposeFULL_tot = rowSums(surveys_scored[,  grepl("\\bPWB_3\\b|\\bPWB_7_R\\b|\\bPWB_10_R\\b|\\bPWB_Purpose2_R\\b|\\bPWB_Purpose3_R\\b|\\bPWB_Purpose1\\b|\\bPWB_Purpose4\\b", names(surveys_scored))]))

#Self-Acceptance Q1, Q2, Q5R
surveys_scored <- cbind(surveys_scored, PWB_SelfAccept_tot = rowSums(surveys_scored[,  grepl("\\bPWB_1\\b|\\bPWB_2\\b|\\bPWB_5_R\\b", names(surveys_scored))]))

# assign label to subscale sum score
attr(surveys_scored$PWB_Autonomy_tot, "label") <- "PWB18 Autonomy Subscale Sum Score"
attr(surveys_scored$PWB_EnvMas_tot, "label") <- "PWB18 Environmental Subscale Sum Score"
attr(surveys_scored$PWB_Growth_tot, "label") <- "PWB18 Growth Subscale Sum Score"
attr(surveys_scored$PWB_PosRelations_tot, "label") <- "PWB18 ositive Relations with Others Subscale Sum Score"
attr(surveys_scored$PWB_Purpose_tot, "label") <- "PWB18 Purpose in Life Subscale Sum Score"
#attr(surveys_scored$PWB_PurposeFULL_tot, "label") <- "PWB42 Purpose in Life Subscale Sum Score"
attr(surveys_scored$PWB_SelfAccept_tot, "label") <- "PWB18 Acceptance Subscale Sum Score"

```

## SRIS
Items 2,3,7,8,9 reverse scored. 
The self-reflection subscale items are 1,4,6,10,11,12. The insight subscale items are 2R,3R,5,7R,8R,9R
```{r}
surveys_scored <- surveys_scored %>% #reverse score
  mutate(SRIS12_2_R = 8 - SRIS12_2,
         SRIS12_3_R = 8 - SRIS12_3,
         SRIS12_7_R = 8 - SRIS12_7,
         SRIS12_8_R = 8 - SRIS12_8,
         SRIS12_9_R = 8 - SRIS12_9) 

#self-reflection 
surveys_scored <- cbind(surveys_scored, SRIS_reflection_tot = rowSums(surveys_scored[,  grepl("\\bSRIS12_1\\b|\\bSRIS12_4\\b|\\bSRIS12_6\\b|\\bSRIS12_10\\b|\\bSRIS12_11\\b|\\bSRIS12_12\\b", names(surveys_scored))]))

surveys_scored <- cbind(surveys_scored, SRIS_insight_tot = rowSums(surveys_scored[,  grepl("\\bSRIS12_2_R\\b|\\bSRIS12_3_R\\b|\\bSRIS12_5\\b|\\bSRIS12_7_R\\b|\\bSRIS12_8_R\\b|\\bSRIS12_9_R\\b", names(surveys_scored))]))

attr(surveys_scored$SRIS_reflection_tot, "label") <- "SRIS Self Reflection Scale Sum Score"
attr(surveys_scored$SRIS_insight_tot, "label") <- "SRIS Self Insight Scale Sum Score"

```

## WB-pro
No scoring here, just renaming columns to identify the dimension assessed. 
```{r}
wbpro_names_old <- colnames(surveys_scored[grepl("WBPro_",names(surveys_scored))])
wbpro_names_new <- c("WBpro_Autonomy", "WBpro_Clear Thinking", "WBpro_Competence", "WBpro_Emotional Stability", "WBpro_Empathy", "WBpro_Engagement", "WBpro_Meaning", "WBpro_Optimism", "WBpro_Positive Emotions","WBpro_Positive Relationships", "WBpro_Prosocial Behavior", "WBpro_Resilience", "WBpro_Self-Acceptance", "WBpro_Self-Esteem", "WBpro_Vitality")
surveys_scored <- surveys_scored %>% rename_with(~ wbpro_names_new, all_of(wbpro_names_old))
```

## ANIQ
Awareness subscale: Items 1-5
Temporal coherence subscale: Items 6-10
Causal coherence subscale: Items 11-15
Thematic coherence subscale: Items 16-20

Items within each subscale are summed, with a possible range of 0-50. Higher scores indicate greater awareness/coherence, respectively
```{r}

surveys_scored <- cbind(surveys_scored, ANIQ_awareness_tot = rowSums(surveys_scored[,  grepl("\\bANIQ_1\\b|\\bANIQ_2\\b|\\bANIQ_3\\b|\\bANIQ_4\\b|\\bANIQ_5\\b", names(surveys_scored))]))

surveys_scored <- cbind(surveys_scored, ANIQ_temp_coherence_tot = rowSums(surveys_scored[,  grepl("\\bANIQ_6\\b|\\bANIQ_7\\b|\\bANIQ_8\\b|\\bANIQ_9\\b|\\bANIQ_10\\b", names(surveys_scored))]))

surveys_scored <- cbind(surveys_scored, ANIQ_causal_coherence_tot = rowSums(surveys_scored[,  grepl("\\bANIQ_11\\b|\\bANIQ_12\\b|\\bANIQ_13\\b|\\bANIQ_14\\b|\\bANIQ_15\\b", names(surveys_scored))]))

surveys_scored <- cbind(surveys_scored, ANIQ_thematic_coherence_tot = rowSums(surveys_scored[,  grepl("\\bANIQ_16\\b|\\bANIQ_17\\b|\\bANIQ_18\\b|\\bANIQ_19\\b|\\bANIQ_20\\b", names(surveys_scored))]))

surveys_scored <- cbind(surveys_scored, ANIQ_tot = rowSums(surveys_scored[,  grepl("\\bANIQ_temp_coherence_tot\\b|\\bANIQ_causal_coherence_tot\\b|\\bANIQ_thematic_coherence_tot\\b|\\bANIQ_awareness_tot\\b", names(surveys_scored))]))


# assign subscale labels 
attr(surveys_scored$ANIQ_awareness_tot, "label") <- "ANIQ Awareness Subscale Sum Score"
attr(surveys_scored$ANIQ_temp_coherence_tot, "label") <- "ANIQ Temporal Coherence Subscale Sum Score"
attr(surveys_scored$ANIQ_causal_coherence_tot, "label") <- "ANIQ Causal Coherence Subscale Sum Score"
attr(surveys_scored$ANIQ_thematic_coherence_tot, "label") <- "ANIQ Thematic Coherence Subscale Sum Score"
attr(surveys_scored$ANIQ_tot, "label") <- "ANIQ Total Score"

```

# add labels

```{r}
# reassign attributions for variables included in the 
for (col_name in varname[multi_resposne_idx]) {
  attributes(surveys_scored[[col_name]]) <- attributes(survey_tidy[[col_name]])
}

# assign the same label to recording variables
for (col_name in c(paste0("SDOH_", 1:7), paste0("CH_3_15_", 1:13), "CH_1", "CH_2")){
  attr(surveys_scored[[paste0(col_name, "_R")]], "label") <- attr(surveys_scored[[col_name]], "label")
}
```


# export cleaned dataset
```{r b_edit}
surveys_scored_dict <- create_dictionary(surveys_scored, remove_repeated = F, use_references = F)


#write.csv(surveys_scored, here("outputs", "ltaw_data_scored.csv"), row.names = F)
#write.csv(surveys_scored_dict, here("outputs", "ltaw_data_scored_dict.csv"), row.names = F)
```